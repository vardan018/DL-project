{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526},{"sourceId":10121862,"sourceType":"datasetVersion","datasetId":6245757}],"dockerImageVersionId":30043,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction \nAutoencoder are special type of deep learning architecture that consist of two networks encoder and decoder.\nThe encoder, through a series of CNN and downsampling, learns a reduced dimensional representation of the input data while decoder  through the use of CNN and upsampling, attempts to regenerate the data from the these representations. A well-trained decoder is able to regenerated data that is identical or as close as possible to the original input data.\nAutoencoder are generally used for anamoly detection, denoising image, colorizing the images. Here, i am going to colorize the landscape images using autoencoder.","metadata":{}},{"cell_type":"markdown","source":"<img src = 'https://miro.medium.com/max/600/1*nqzWupxC60iAH2dYrFT78Q.png' >","metadata":{}},{"cell_type":"markdown","source":"## Image Colorization\nImage colorization using different softwares require large amount of human effort, time and skill.But special type of deep learning architecture called autoencoder has made this task quiet easy. Automatic image colorization often involves the use of a class of convolutional neural networks (CNN) called autoencoders. These neural networks are able to distill the salient features of an image, and then regenerate the image based on these learned features. ","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://tinyclouds.org/colorize/best/6.jpg\">","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport cv2\nfrom keras.layers import MaxPool2D,Conv2D,UpSampling2D,Input,Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import img_to_array\nimport os\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:19:19.419070Z","iopub.execute_input":"2024-12-07T17:19:19.419311Z","iopub.status.idle":"2024-12-07T17:19:24.175440Z","shell.execute_reply.started":"2024-12-07T17:19:19.419287Z","shell.execute_reply":"2024-12-07T17:19:24.174564Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Getting landscape image data,resizing them and appending in array\nTo get the image in sorted order i have defined the function sorted_alphanumeric. Here, I have used open cv library to read and resize images. Finally images are normalized and are converted to array and are appended in empty list","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\n# Helper function for alphanumeric sorting\ndef sorted_alphanumeric(data):\n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    return sorted(data, key=alphanum_key)\n\n# Defining the size of the image\nSIZE = 160\ncolor_img = []\ngray_img = []\n\n# Path to color images\ncolor_path = '/kaggle/input/imagenetsubsub'\ncolor_files = os.listdir(color_path)\ncolor_files = sorted_alphanumeric(color_files)\n\n# Processing color and grayscale images\nfor file_name in tqdm(color_files):\n    if file_name == 'ILSVRC2012_test_00080098':\n        break\n    else:\n        # Read color image\n        color_image = cv2.imread(os.path.join(color_path, file_name), 1)\n        \n        if color_image is None:\n            continue  # Skip if the file is not a valid image\n\n        # Convert to RGB\n        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n        # Resize image\n        color_image = cv2.resize(color_image, (SIZE, SIZE))\n        # Normalize and append to color images\n        color_image = color_image.astype('float32') / 255.0\n        color_img.append(img_to_array(color_image))\n\n        # Convert to grayscale\n        gray_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n        # Expand dimensions to match the expected shape (H, W, 1)\n        gray_image = np.expand_dims(gray_image, axis=-1)\n        # Append to grayscale images\n        gray_img.append(img_to_array(gray_image))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:24:35.632402Z","iopub.execute_input":"2024-12-07T17:24:35.632689Z","iopub.status.idle":"2024-12-07T17:25:01.423806Z","shell.execute_reply.started":"2024-12-07T17:24:35.632665Z","shell.execute_reply":"2024-12-07T17:25:01.422982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print shapes to confirm processing\nprint(\"Color images shape:\", len(color_img))\nprint(\"Grayscale images shape:\", len(gray_img))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:25:01.425840Z","iopub.execute_input":"2024-12-07T17:25:01.426248Z","iopub.status.idle":"2024-12-07T17:25:01.430704Z","shell.execute_reply.started":"2024-12-07T17:25:01.426210Z","shell.execute_reply":"2024-12-07T17:25:01.429864Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plotting Color image and it's corresponding grayscale image","metadata":{}},{"cell_type":"code","source":"def plot_images(color, grayscale):\n    plt.figure(figsize=(15, 15))\n    plt.subplot(1, 2, 1)\n    plt.title('Color Image', color='green', fontsize=20)\n    plt.imshow(color)\n    plt.subplot(1, 2, 2)\n    plt.title('Grayscale Image', color='black', fontsize=20)\n    plt.imshow(grayscale.squeeze(), cmap='gray')  # Squeeze to remove extra dimension\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:25:01.431773Z","iopub.execute_input":"2024-12-07T17:25:01.432024Z","iopub.status.idle":"2024-12-07T17:25:01.442512Z","shell.execute_reply.started":"2024-12-07T17:25:01.432001Z","shell.execute_reply":"2024-12-07T17:25:01.441943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plotting image pair**","metadata":{}},{"cell_type":"code","source":"for i in range(3,10):\n     plot_images(color_img[i],gray_img[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:25:01.443459Z","iopub.execute_input":"2024-12-07T17:25:01.443676Z","iopub.status.idle":"2024-12-07T17:25:04.259969Z","shell.execute_reply.started":"2024-12-07T17:25:01.443654Z","shell.execute_reply":"2024-12-07T17:25:04.259316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Slicing and reshaping\nOut of 5000 images I have sliced them to two part. train images consist 4000 images  while test images contains 1000 images.\nAfter slicing the image array, I reshaped them so that images can be fed directly into our encoder network","metadata":{}},{"cell_type":"code","source":"train_gray_image = gray_img[:3000]\ntrain_color_image = color_img[:3000]\n\ntest_gray_image = gray_img[3000:]\ntest_color_image = color_img[3000:]\n\n# Ensuring the shapes match without reshaping unnecessarily\ntrain_g = np.array(train_gray_image)  # Grayscale: (3000, SIZE, SIZE, 1)\ntrain_c = np.array(train_color_image)  # Color: (3000, SIZE, SIZE, 3)\n\nprint('Train gray image shape:', train_g.shape)\nprint('Train color image shape:', train_c.shape)\n\ntest_g = np.array(test_gray_image)  # Grayscale: (remaining, SIZE, SIZE, 1)\ntest_c = np.array(test_color_image)  # Color: (remaining, SIZE, SIZE, 3)\n\nprint('Test gray image shape:', test_g.shape)\nprint('Test color image shape:', test_c.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:56:17.576588Z","iopub.execute_input":"2024-12-07T17:56:17.576866Z","iopub.status.idle":"2024-12-07T17:56:18.031334Z","shell.execute_reply.started":"2024-12-07T17:56:17.576842Z","shell.execute_reply":"2024-12-07T17:56:18.030534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining our model\nEncoder layer of our model consist blocks of Convolution layer with different number of kernel and kernel_size. Here, Convolution is used for downsampling.\nSimilary, Decoder layer of our model consist of  transpose convolution layer with different kernel size. Here, Decoder layer upsample image downsampled by encoder.\nSince there is feature loss between the encoder and decoder layers so inorder to prevent feature loss i have concatenate corresponding encoder and decoder layers. Check U_Net architecture for better understanding......","metadata":{}},{"cell_type":"code","source":"from keras import layers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.applications import VGG16\nimport tensorflow.keras.backend as K\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:56:19.408496Z","iopub.execute_input":"2024-12-07T17:56:19.408769Z","iopub.status.idle":"2024-12-07T17:56:19.412780Z","shell.execute_reply.started":"2024-12-07T17:56:19.408744Z","shell.execute_reply":"2024-12-07T17:56:19.411777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def down(filters, kernel_size, apply_batch_normalization=True):\n    downsample = tf.keras.models.Sequential()\n    downsample.add(layers.Conv2D(filters, kernel_size, padding='same', strides=2))\n    if apply_batch_normalization:\n        downsample.add(layers.BatchNormalization())\n    downsample.add(layers.LeakyReLU())\n    return downsample\n\ndef up(filters, kernel_size, dropout=False):\n    upsample = tf.keras.models.Sequential()\n    upsample.add(layers.Conv2DTranspose(filters, kernel_size, padding='same', strides=2))\n    if dropout:\n        upsample.add(layers.Dropout(0.2))\n    upsample.add(layers.LeakyReLU())\n    return upsample\n\ndef build_generator():\n    inputs = layers.Input(shape=[160, 160, 3])\n    \n    # Downsampling\n    d1 = down(128, (3, 3), False)(inputs)\n    d2 = down(128, (3, 3), False)(d1)\n    d3 = down(256, (3, 3), True)(d2)\n    d4 = down(512, (3, 3), True)(d3)\n    d5 = down(512, (3, 3), True)(d4)\n    \n    # Upsampling\n    u1 = up(512, (3, 3), False)(d5)\n    u1 = layers.concatenate([u1, d4])\n    u2 = up(256, (3, 3), False)(u1)\n    u2 = layers.concatenate([u2, d3])\n    u3 = up(128, (3, 3), False)(u2)\n    u3 = layers.concatenate([u3, d2])\n    u4 = up(128, (3, 3), False)(u3)\n    u4 = layers.concatenate([u4, d1])\n    u5 = up(3, (3, 3), False)(u4)\n    u5 = layers.concatenate([u5, inputs])\n    output = layers.Conv2D(3, (2, 2), strides=1, padding='same', activation='tanh')(u5)\n    \n    return Model(inputs, output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:56:19.848693Z","iopub.execute_input":"2024-12-07T17:56:19.848955Z","iopub.status.idle":"2024-12-07T17:56:19.860707Z","shell.execute_reply.started":"2024-12-07T17:56:19.848929Z","shell.execute_reply":"2024-12-07T17:56:19.859842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_discriminator():\n    inputs = layers.Input(shape=(160, 160, 6))  # Grayscale + Color image\n    x = layers.Conv2D(64, (4, 4), strides=2, padding='same')(inputs)\n    x = layers.LeakyReLU()(x)\n    \n    x = layers.Conv2D(128, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU()(x)\n    \n    x = layers.Conv2D(256, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU()(x)\n    \n    x = layers.Conv2D(512, (4, 4), strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU()(x)\n    \n    x = layers.Flatten()(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    return Model(inputs, x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:56:20.238324Z","iopub.execute_input":"2024-12-07T17:56:20.238563Z","iopub.status.idle":"2024-12-07T17:56:20.245651Z","shell.execute_reply.started":"2024-12-07T17:56:20.238536Z","shell.execute_reply":"2024-12-07T17:56:20.244905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load VGG16 pre-trained model\nvgg = VGG16(weights='imagenet', include_top=False, input_shape=(160, 160, 3))\nfeature_extractor = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\nfeature_extractor.trainable = False  # Freeze VGG16 layers\n\ndef perceptual_loss(y_true, y_pred):\n    # Ensure inputs are 4D tensors\n    y_true = tf.reshape(y_true, (-1, 160, 160, 3))\n    y_pred = tf.reshape(y_pred, (-1, 160, 160, 3))\n    \n    # Normalize inputs to match VGG expectations\n    y_true = y_true * 255.0  # Scale to [0, 255]\n    y_pred = y_pred * 255.0  # Scale to [0, 255]\n\n    # Extract VGG features\n    y_true_features = feature_extractor(y_true)\n    y_pred_features = feature_extractor(y_pred)\n\n    # Compute perceptual loss as MSE in feature space\n    return K.mean(K.square(y_true_features - y_pred_features))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:57:03.471545Z","iopub.execute_input":"2024-12-07T17:57:03.471817Z","iopub.status.idle":"2024-12-07T17:57:03.738010Z","shell.execute_reply.started":"2024-12-07T17:57:03.471793Z","shell.execute_reply":"2024-12-07T17:57:03.737398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = build_generator()\ndiscriminator = build_discriminator()\n\n# Compile the discriminator\ndiscriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n\n# Combined model: Grayscale input -> Generator -> Discriminator\ngrayscale_input = layers.Input(shape=(160, 160, 3))\ncolor_output = generator(grayscale_input)\n\n# Combine grayscale + generated color image\ncombined_input = layers.Concatenate()([grayscale_input, color_output])\nvalidity = discriminator(combined_input)\n\ncombined = Model(grayscale_input, validity)\ncombined.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n                 loss=perceptual_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:57:04.269750Z","iopub.execute_input":"2024-12-07T17:57:04.270032Z","iopub.status.idle":"2024-12-07T17:57:04.811614Z","shell.execute_reply.started":"2024-12-07T17:57:04.270005Z","shell.execute_reply":"2024-12-07T17:57:04.810960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nepochs = 50\nbatch_size = 50\n\nreal = np.ones((batch_size, 1))  # Real labels\nfake = np.zeros((batch_size, 1))  # Fake labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:57:04.891872Z","iopub.execute_input":"2024-12-07T17:57:04.892111Z","iopub.status.idle":"2024-12-07T17:57:04.896387Z","shell.execute_reply.started":"2024-12-07T17:57:04.892089Z","shell.execute_reply":"2024-12-07T17:57:04.895615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    for batch in range(0, len(train_g_rgb), batch_size):\n        # Get a batch of grayscale and color images\n        gray_batch = train_g_rgb[batch:batch + batch_size]  # (batch_size, 160, 160, 3)\n        color_batch = train_c[batch:batch + batch_size]     # (batch_size, 160, 160, 3)\n\n        # Skip incomplete batches\n        if gray_batch.shape[0] < batch_size:\n            continue\n\n        # Generate fake color images\n        fake_color_batch = generator.predict(gray_batch)\n\n        # Train the discriminator\n        real_combined = np.concatenate([gray_batch, color_batch], axis=-1)  # Shape: (batch_size, 160, 160, 6)\n        fake_combined = np.concatenate([gray_batch, fake_color_batch], axis=-1)  # Shape: (batch_size, 160, 160, 6)\n\n        d_loss_real = discriminator.train_on_batch(real_combined, real)\n        d_loss_fake = discriminator.train_on_batch(fake_combined, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator (use perceptual loss)\n        g_loss = combined.train_on_batch(gray_batch, color_batch)\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:57:05.631690Z","iopub.execute_input":"2024-12-07T17:57:05.631954Z","iopub.status.idle":"2024-12-07T17:57:07.156295Z","shell.execute_reply.started":"2024-12-07T17:57:05.631924Z","shell.execute_reply":"2024-12-07T17:57:07.154054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"gray_batch shape: {gray_batch.shape}\")\nprint(f\"color_batch shape: {color_batch.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:55:14.096227Z","iopub.execute_input":"2024-12-07T17:55:14.096520Z","iopub.status.idle":"2024-12-07T17:55:14.100751Z","shell.execute_reply.started":"2024-12-07T17:55:14.096494Z","shell.execute_reply":"2024-12-07T17:55:14.099836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fitting our model","metadata":{}},{"cell_type":"code","source":"\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='mean_absolute_error',\n              metrics=['acc'])\n\nmodel.fit(train_g_rgb, train_c, epochs=50, batch_size=50, verbose=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:27:18.414179Z","iopub.execute_input":"2024-12-07T17:27:18.414494Z","iopub.status.idle":"2024-12-07T17:41:48.287019Z","shell.execute_reply.started":"2024-12-07T17:27:18.414464Z","shell.execute_reply":"2024-12-07T17:41:48.286217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Convert to NumPy arrays\ntest_color_image = np.array(test_color_image)\ntest_g_rgb = np.array(test_g_rgb)\n\n# Check shapes\nprint(\"Shape of test_g_rgb:\", test_g_rgb.shape)\nprint(\"Shape of test_color_image:\", test_color_image.shape)\n\n# Ensure equal number of samples\nmin_samples = min(test_g_rgb.shape[0], test_color_image.shape[0])\ntest_g_rgb = test_g_rgb[:min_samples]\ntest_color_image = test_color_image[:min_samples]\n\n# Valid\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:44:21.028552Z","iopub.execute_input":"2024-12-07T17:44:21.028828Z","iopub.status.idle":"2024-12-07T17:44:21.196733Z","shell.execute_reply.started":"2024-12-07T17:44:21.028805Z","shell.execute_reply":"2024-12-07T17:44:21.195962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, acc = model.evaluate(test_g_rgb, test_color_image)\nprint(f\"Loss: {loss}, Accuracy: {acc}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:44:44.314551Z","iopub.execute_input":"2024-12-07T17:44:44.314828Z","iopub.status.idle":"2024-12-07T17:44:47.377805Z","shell.execute_reply.started":"2024-12-07T17:44:44.314804Z","shell.execute_reply":"2024-12-07T17:44:47.377011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# plotting colorized image along with grayscale and color image","metadata":{}},{"cell_type":"code","source":"print(f\"Shape of test_gray_image[{i}]:\", test_gray_image[i].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:45:56.972005Z","iopub.execute_input":"2024-12-07T17:45:56.972285Z","iopub.status.idle":"2024-12-07T17:45:56.976627Z","shell.execute_reply.started":"2024-12-07T17:45:56.972259Z","shell.execute_reply":"2024-12-07T17:45:56.975962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import structural_similarity as ssim\nimport numpy as np\n\n# Define accuracy calculation function\ndef calculate_metrics(color, predicted):\n    mse = np.mean((color - predicted) ** 2)\n    psnr = 20 * np.log10(1.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n    ssim_value = ssim(color, predicted, multichannel=True, data_range=color.max() - color.min())\n    return mse, psnr, ssim_value\n\n# Modified plotting function with accuracy metrics\ndef plot_images_with_metrics(color, grayscale, predicted, mse, psnr, ssim_value):\n    plt.figure(figsize=(15,15))\n    plt.subplot(1,3,1)\n    plt.title('Color Image', color='green', fontsize=20)\n    plt.imshow(color)\n    plt.subplot(1,3,2)\n    plt.title('Grayscale Image', color='black', fontsize=20)\n    plt.imshow(grayscale, cmap='gray')\n    plt.subplot(1,3,3)\n    plt.title(f'Predicted Image\\nMSE: {mse:.4f}\\nPSNR: {psnr:.2f} dB\\nSSIM: {ssim_value:.4f}', color='red', fontsize=16)\n    plt.imshow(predicted)\n    plt.show()\n\nresults = []\n# Loop to display images and their metrics\n# Loop to display images and their metrics\nfor i in range(50, 58):\n    # Fix shape: convert (160, 160, 1) -> (160, 160, 3)\n    gray_image = np.repeat(test_gray_image[i], 3, axis=-1)  # Duplicate channel to make it RGB\n    gray_image = gray_image.reshape(1, SIZE, SIZE, 3)  # Add batch dimension\n\n    # Predict\n    predicted = model.predict(gray_image)\n\n    # Ensure predicted output has the correct shape\n    if predicted.shape[-1] == 1:  # If the output is single-channel\n        predicted = np.repeat(predicted, 3, axis=-1)  # Convert to 3 channels\n\n    predicted = predicted.squeeze()  # Remove batch dimension\n    predicted = np.clip(predicted, 0.0, 1.0)  # Clip values to [0, 1]\n\n    # Calculate metrics\n    mse, psnr, ssim_value = calculate_metrics(test_color_image[i], predicted)\n    results.append([mse, psnr, ssim_value])\n\n    # Plot results\n    plot_images_with_metrics(test_color_image[i], test_gray_image[i].squeeze(), predicted, mse, psnr, ssim_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:46:28.461132Z","iopub.execute_input":"2024-12-07T17:46:28.461414Z","iopub.status.idle":"2024-12-07T17:46:32.698095Z","shell.execute_reply.started":"2024-12-07T17:46:28.461390Z","shell.execute_reply":"2024-12-07T17:46:32.697301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = np.array(results)\n\n# Calculate average metrics\naverage_metrics = results.mean(axis=0)\n\n# Output the average metrics\naverage_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T17:41:53.321325Z","iopub.status.idle":"2024-12-07T17:41:53.321864Z","shell.execute_reply":"2024-12-07T17:41:53.321589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thanks for your visit.\n## Any suggestions to improve this model is highly appreciated.\n# Feel free to  comment","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
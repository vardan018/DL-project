{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6799,
          "databundleVersionId": 4225553,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "script",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook8f632dcecf 4175ec",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "SChoa5UYJjzy"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "imagenet_object_localization_challenge_path = kagglehub.competition_download('imagenet-object-localization-challenge')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "S1C6JugbJjz1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-06T16:14:06.684983Z\",\"iopub.execute_input\":\"2024-12-06T16:14:06.685237Z\",\"iopub.status.idle\":\"2024-12-06T16:14:53.775822Z\",\"shell.execute_reply.started\":\"2024-12-06T16:14:06.685210Z\",\"shell.execute_reply\":\"2024-12-06T16:14:53.774826Z\"}}\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination directories\n",
        "source_dir = '/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test'\n",
        "destination_dir = '/kaggle/working/images'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# List all the files in the source directory\n",
        "files = os.listdir(source_dir)\n",
        "\n",
        "# Limit the total size to 500 MB\n",
        "total_size = 0\n",
        "for file in files:\n",
        "    file_path = os.path.join(source_dir, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        total_size += os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "        if total_size <= 500:\n",
        "            # Copy file to the destination directory\n",
        "            shutil.copy(file_path, destination_dir)\n",
        "        else:\n",
        "            print(f\"Reached 500 MB limit. Stopping the file copy process.\")\n",
        "            break\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# # Colorization Test Model\n",
        "# ## This program will load an already-trained model, run it against a test set, and save individual test images\n",
        "# ## Output will be saved in ./Test_Output\n",
        "\n",
        "# %% [markdown]\n",
        "# the first code in this note book copies 500m worth of images inn /kaggle/working rest codes are from your notebook\n",
        "#\n",
        "\n",
        "# %% [markdown]\n",
        "# # Things to do 1. change paths in the code\n",
        "# the images we will be working with will be stored in /kaggle/working\n",
        "# the pretained model weights are in\n",
        "# /kaggle/input/pretrainedmodel/other/default/1/filename\n",
        "# you might need to use tensorflow.keras instead of keras\n",
        "# see whereever there is path you modify these things then the code should run.\n",
        "# make sure code is running on gpu this by adding a device = cuda code and selecting gpu T4 x2 from session options accelaerator\n",
        "#\n",
        "\n",
        "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-06T16:14:53.777702Z\",\"iopub.execute_input\":\"2024-12-06T16:14:53.778131Z\",\"iopub.status.idle\":\"2024-12-06T16:14:55.205754Z\",\"shell.execute_reply.started\":\"2024-12-06T16:14:53.778089Z\",\"shell.execute_reply\":\"2024-12-06T16:14:55.204925Z\"}}\n",
        "import os\n",
        "import zipfile\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# Specify the directory and zip file name\n",
        "dir_to_zip = \"/kaggle/working/images\"\n",
        "zip_file_name = \"images.zip\"\n",
        "\n",
        "# Create a zip file\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    for file in os.listdir(dir_to_zip):\n",
        "        file_path = os.path.join(dir_to_zip, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            # Add file to the zip archive\n",
        "            zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "# Display link to download the zip file\n",
        "print(f\"Zip file created: {zip_file_name}\")\n",
        "FileLink(zip_file_name)\n",
        "\n",
        "\n",
        "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-06T16:03:17.848712Z\",\"iopub.execute_input\":\"2024-12-06T16:03:17.849381Z\",\"iopub.status.idle\":\"2024-12-06T16:03:29.758246Z\",\"shell.execute_reply.started\":\"2024-12-06T16:03:17.849337Z\",\"shell.execute_reply\":\"2024-12-06T16:03:29.757105Z\"}}\n",
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.engine.saving import load_model\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "%pylab inline\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tnrange, tqdm_notebook, tqdm\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import random\n",
        "\n",
        "# %% [code]\n",
        "def list_image_files(directory):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]\n",
        "\n",
        "def is_an_image_file(filename):\n",
        "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
        "    for ext in IMAGE_EXTENSIONS:\n",
        "        if ext in filename:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# %% [code]\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path[0])\n",
        "\n",
        "    # Make sure all images are 256 x 256 by cropping them\n",
        "    r, c = img.shape[:2]\n",
        "    r_diff = (r - 256) // 2\n",
        "    c_diff = (c - 256) // 2\n",
        "    cropped = img[r_diff:256 + r_diff, c_diff:256 + c_diff]\n",
        "    return cropped\n",
        "\n",
        "def load_images(path, n_images=-1, shouldShuffle=False):\n",
        "    all_image_paths = list_image_files(path)\n",
        "    if shouldShuffle:\n",
        "        random.shuffle(all_image_paths)\n",
        "\n",
        "    if n_images < 0:\n",
        "        n_images = len(all_image_paths)\n",
        "    images_l, images_ab = [], []\n",
        "\n",
        "    # Initialize a progress bar with max of n_images\n",
        "    pbar = tqdm_notebook(total = n_images, desc=\"Loading Images...\")\n",
        "\n",
        "    for path in zip(all_image_paths):\n",
        "        img = load_image(path)\n",
        "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        lab_img = preprocess_image(lab_img)\n",
        "\n",
        "        l = lab_img[:,:,0]\n",
        "        l = l[:,:,np.newaxis]\n",
        "        # Include all 3 channels, overwrite 1st channel with 0's\n",
        "        ab = lab_img[:,:,1:]\n",
        "\n",
        "        images_l.append(l)\n",
        "        images_ab.append(ab)\n",
        "\n",
        "        images_loaded = len(images_l)\n",
        "\n",
        "        # Increase progress by one\n",
        "        pbar.update(1)\n",
        "\n",
        "        if images_loaded > n_images - 1:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        'l': np.array(images_l),\n",
        "        'ab': np.array(images_ab)\n",
        "    }\n",
        "\n",
        "# %% [code]\n",
        "RESHAPE = (256,256)\n",
        "\n",
        "def preprocess_image(cv_img):\n",
        "    img = (cv_img - 127.5) / 127.5\n",
        "    return img\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = (img * 127.5) + 127.5\n",
        "    return img.astype('uint8')\n",
        "\n",
        "# %% [code]\n",
        "def save_image(np_arr, path):\n",
        "    img = np_arr * 127.5 + 127.5\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(path)\n",
        "\n",
        "# %% [code]\n",
        "def get_generator(H, W, k):\n",
        "    # Inputs: height and width of the input image\n",
        "    # Returns the model, which generates the AB channels\n",
        "\n",
        "    # Pix2pix adapted from\n",
        "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
        "\n",
        "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "        \"\"\"Layers used during downsampling\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "\n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "        \"\"\"Layers used during upsampling\"\"\"\n",
        "        u = UpSampling2D(size=2)(layer_input)\n",
        "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "        if dropout_rate:\n",
        "            u = Dropout(dropout_rate)(u)\n",
        "        u = BatchNormalization(momentum=0.8)(u)\n",
        "        u = Concatenate()([u, skip_input])\n",
        "        return u\n",
        "\n",
        "    gf = 64 # Number of filters in the first layer of G\n",
        "\n",
        "    noise_in = Input(shape=(100,))\n",
        "    condition_in = Input(shape=(H, W, 1))\n",
        "\n",
        "    # pass noise through a FC layer to get it to the right size\n",
        "    noise = Dense(H * H)(noise_in)\n",
        "\n",
        "    # reshape to be the size of an image channel\n",
        "    noise = Reshape((H, H, 1))(noise)\n",
        "\n",
        "    # stick the (somewhat modified) noise as the second channel after\n",
        "    # the gray input. Assuming new dimension of hid will be\n",
        "    # B x 256 x 256 x 2, where B is the batch size.\n",
        "    d0 = Concatenate(axis=-1)([condition_in, noise])\n",
        "#     d0 = condition_in # Don't need noise since it's being ignored anyway\n",
        "\n",
        "    # U-NET\n",
        "    # Downsampling\n",
        "    d1 = conv2d(d0, gf, bn=False)\n",
        "    d2 = conv2d(d1, gf*2)\n",
        "    d3 = conv2d(d2, gf*4)\n",
        "    d4 = conv2d(d3, gf*8)\n",
        "    d5 = conv2d(d4, gf*8)\n",
        "    d6 = conv2d(d5, gf*8)\n",
        "    d7 = conv2d(d6, gf*8)\n",
        "\n",
        "    # Upsampling\n",
        "    u1 = deconv2d(d7, d6, gf*8)\n",
        "    u2 = deconv2d(u1, d5, gf*8)\n",
        "    u3 = deconv2d(u2, d4, gf*8)\n",
        "    u4 = deconv2d(u3, d3, gf*4)\n",
        "    u5 = deconv2d(u4, d2, gf*2)\n",
        "    u6 = deconv2d(u5, d1, gf)\n",
        "\n",
        "    u7 = UpSampling2D(size=2)(u6)\n",
        "\n",
        "    # Final 2-channel AB image with values between -1 and 1\n",
        "    img_out = Conv2D(2*k, kernel_size=4, strides=1, padding='same', activation='tanh', name='pred_ab')(u7)\n",
        "\n",
        "    # Make Model\n",
        "    model = Model(inputs=[noise_in, condition_in], outputs=img_out)\n",
        "\n",
        "    # Show summary of layers\n",
        "    print(\"Generator Model:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "def get_discriminator(H, W, k):\n",
        "    # Inputs: height and width of the input image\n",
        "    # Returns the model, which predicts real/fake\n",
        "    # over a set of spatial regions (i.e., predicts a matrix instead of a scalar).\n",
        "\n",
        "    # Pix2pix adapted from\n",
        "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
        "\n",
        "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "\n",
        "    # Number of filters in the first layer of D\n",
        "    df = 64\n",
        "\n",
        "    img_in = Input(shape=(H, W, 2*k)) # AB channels\n",
        "    condition_in = Input(shape=(H, W, 1)) # L channel\n",
        "\n",
        "    # Concat the L and AB channels\n",
        "    concat_imgs = Concatenate()([condition_in, img_in])\n",
        "\n",
        "    d1 = d_layer(concat_imgs, df, bn=False)\n",
        "    d2 = d_layer(d1, df*2)\n",
        "    d3 = d_layer(d2, df*4)\n",
        "    d4 = d_layer(d3, df*8)\n",
        "\n",
        "    # validity map is a one-channel matrix 1/16 the size of the input (halved 4 times).\n",
        "    # Each number predicts whether a region of the input is real/fake.\n",
        "    validity = Conv2D(1*k, kernel_size=4, strides=1, padding='same', name='pred_valid')(d4)\n",
        "\n",
        "    # Build Model\n",
        "    model = Model(inputs=[img_in, condition_in], outputs=validity)\n",
        "\n",
        "    # Show summary of layers\n",
        "    print(\"Disciminator Model:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# %% [code]\n",
        "def min_k_diff(y_true, y_pred):\n",
        "    # Shape: (Batch, H, W, k, 2)\n",
        "    y_true = K.reshape(y_true, (-1, H, W, k, 2))\n",
        "    y_pred = K.reshape(y_pred, (-1, H, W, k, 2))\n",
        "\n",
        "    print(\"true:\", y_true.shape)\n",
        "    print(\"pred:\", y_pred.shape)\n",
        "\n",
        "    diff = y_true - y_pred\n",
        "    diff = K.abs(diff)\n",
        "    diff = K.mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
        "\n",
        "    loss_metric = diff\n",
        "\n",
        "    min_for_each_batch = K.min(loss_metric, axis=1)\n",
        "    return K.sum(min_for_each_batch) #* .01\n",
        "\n",
        "# %% [code]\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def generate_noise(n_samples, noise_dim):\n",
        "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
        "    return X\n",
        "\n",
        "# %% [code]\n",
        "def save_rgb_img(l, ab, filename):\n",
        "    # Make sure ab is the right type, generated imgs change to float32\n",
        "    ab = ab.astype(np.float64)\n",
        "\n",
        "    # Merge\n",
        "    merged = cv2.merge((l, ab))\n",
        "\n",
        "    # Get between 0, 255\n",
        "    deprocessed = deprocess_image(merged)\n",
        "\n",
        "    # Change to BGR (Curse you CV2!!!)\n",
        "    rgb = cv2.cvtColor(deprocessed, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Save\n",
        "    cv2.imwrite(save_path + filename, rgb)\n",
        "\n",
        "# %% [code]\n",
        "def make_discrim_models():\n",
        "    # Load training data\n",
        "    max_imgs = 2500\n",
        "    if len(list_image_files(train_dataset)) > max_imgs:\n",
        "        num_imgs = max_imgs\n",
        "    else:\n",
        "        num_imgs = -1\n",
        "\n",
        "    train_data = load_images(train_dataset, num_imgs, True)\n",
        "    train_l, train_ab = train_data['l'], train_data['ab']\n",
        "\n",
        "    # Make generated data\n",
        "    noise = generate_noise(len(train_l), 100)\n",
        "    train_predictions = generator.predict([noise, train_l])\n",
        "\n",
        "    # Tile truth data\n",
        "    tiled = np.tile(train_ab, k)\n",
        "\n",
        "    # Make discrim predictions\n",
        "    generated_discrim_values = discriminator.predict([train_predictions, train_l])\n",
        "    true_discrim_values = discriminator.predict([tiled, train_l])\n",
        "\n",
        "    # Flatten discrim values\n",
        "    n = generated_discrim_values.shape\n",
        "    flat_gen_discrim_val = generated_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
        "    flat_true_discrim_val = true_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
        "\n",
        "    # Make labels\n",
        "    model_labels = np.concatenate((np.zeros(len(generated_discrim_values)), np.ones(len(true_discrim_values))))\n",
        "\n",
        "    # Loop through and store models\n",
        "    models = []\n",
        "    for i in range(k):\n",
        "        model_data = np.concatenate((flat_gen_discrim_val[:,:,i], flat_true_discrim_val[:,:,i]))\n",
        "        model = LogisticRegression(max_iter=1000).fit(model_data, model_labels)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Parameters\n",
        "\n",
        "# %% [code]\n",
        "# Model parameters\n",
        "\n",
        "# Program will grab 100 epoch weights for G,D in ./Output/trained_model_name/\n",
        "# trained_model_name = \"lsun_colorization_full_model\"\n",
        "# trained_model_name = \"places2_colorization_read_imgs_flow_test\"\n",
        "trained_model_name = \"places2_final_k_1_with_noise\"\n",
        "# trained_model_name = 'ablation_circles_equal_l_k_3_no_noise_no_aug'\n",
        "\n",
        "# Program will save output in ./TestOutput/trained_model_name by default\n",
        "# Change from none to save output in ./TestOutput/overwrite_save_dir\n",
        "# overwrite_save_dir = \"WHJ_Predictions\"\n",
        "# overwrite_save_dir = \"LSUN_on_places2\"\n",
        "overwrite_save_dir = None\n",
        "\n",
        "# Should output have the same file names as the test images?\n",
        "preserve_img_names = True\n",
        "\n",
        "# Must match k from model\n",
        "k = 1\n",
        "\n",
        "# Testing parameters\n",
        "num_test_imgs = 100\n",
        "\n",
        "# Should program randomly colorize some test images and leave some ground truth?\n",
        "# This was used to generate data for a user study\n",
        "random_select_gt_or_colorzed = False\n",
        "\n",
        "# Specify what dataset to test on\n",
        "# dataset = 'circle_pairs_equal_l_red_blue/'\n",
        "# dataset = 'new_circles/'\n",
        "# dataset = '../Colorization_GAN/circle_pairs/'\n",
        "# test_dataset = 'lsun/test/'\n",
        "test_dataset = '/kaggle/working/'\n",
        "# train_dataset = 'places2/train/subdir/'\n",
        "# test_dataset = 'places2/10_imgs_per_cat/'\n",
        "# dataset = 'William_Henry_Jackson/WHJ_Resized_Square/'\n",
        "\n",
        "# %% [code]\n",
        "# Find where model is located\n",
        "### modigy these 3 clines below\n",
        "saved_GAN_location = \"\" + trained_model_name + \"/GAN_Weights_Epoch_100.h5\"\n",
        "saved_D_location = \"Output/\" + trained_model_name + \"/Discriminator_Weights_Epoch_100.h5\"\n",
        "\n",
        "# Create folder to store output\n",
        "generic_output_folder = \"Test_Output/\"\n",
        "\n",
        "if overwrite_save_dir is None:\n",
        "    new_output_folder = trained_model_name + \"/\"\n",
        "    save_path = generic_output_folder + new_output_folder\n",
        "else:\n",
        "    save_path = generic_output_folder + overwrite_save_dir + \"/\"\n",
        "\n",
        "if random_select_gt_or_colorzed:\n",
        "    save_path += \"random_colorized_or_ground_truth/\"\n",
        "else:\n",
        "    save_path += \"all_predictions/\"\n",
        "\n",
        "# Ensure output can save in desired location\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# %% [code]\n",
        "# ===================================\n",
        "# COULD NOT HANDLE LARGE TRAINING SET\n",
        "# ===================================\n",
        "\n",
        "# Get training images\n",
        "# Load dataset, convert to LAB, normalize to range [-1, 1]\n",
        "# data = load_images(dataset + 'test', num_test_imgs)\n",
        "# data = load_images(dataset + \"/test/\", num_test_imgs)\n",
        "data = load_images(test_dataset, num_test_imgs)\n",
        "\n",
        "\n",
        "\n",
        "# Only want l channel\n",
        "l_channel_imgs, ab_channel_imgs = data['l'], data['ab']\n",
        "\n",
        "# %% [code] {\"scrolled\":true}\n",
        "# GAN creation\n",
        "H = W = 256\n",
        "\n",
        "# Discriminator loss - MSE seems to produce better results\n",
        "#discrim_loss = 'binary_crossentropy'\n",
        "discrim_loss = 'mse'\n",
        "\n",
        "# 1. Discriminator\n",
        "# Calculate output shape of D (PatchGAN)\n",
        "patch = H // 2**4 # Input size gets cut in half 4 times\n",
        "discriminator = get_discriminator(H, W, k)\n",
        "discriminator.name = 'discrim_model' # Need a name for the loss dictionary below\n",
        "discriminator.compile(optimizer=Adam(2e-4, 0.5), loss=discrim_loss, metrics=['accuracy'])\n",
        "discriminator.trainable = False # For the combined model we will only train the generator\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Generator\n",
        "generator = get_generator(H, W, k)\n",
        "generator.name = 'gen_model' # Need a name for the loss dictionary below\n",
        "\n",
        "# 3. GAN\n",
        "gan_noise_in = Input(shape=(100,))\n",
        "gan_condition_in = Input(shape=(H, W, 1))\n",
        "\n",
        "# By conditioning on L generate a fake version of AB\n",
        "fake_AB = generator([gan_noise_in, gan_condition_in])\n",
        "\n",
        "# Discriminator determines validity of AB images / L pairs\n",
        "print(\"fake_ab:\", fake_AB.shape)\n",
        "\n",
        "print(\"gan_condition_in:\", gan_condition_in.shape)\n",
        "\n",
        "valid = discriminator([fake_AB, gan_condition_in])\n",
        "\n",
        "losses = {'gen_model': min_k_diff, # used to be 'gen_loss'\n",
        "          'discrim_model': discrim_loss}\n",
        "loss_weights = {'gen_model': 100.0, 'discrim_model': 1.0}\n",
        "\n",
        "gan = Model(inputs=[gan_noise_in, gan_condition_in], outputs=[fake_AB, valid])\n",
        "gan.compile(optimizer=Adam(2e-4, 0.5), loss=losses, loss_weights=loss_weights)\n",
        "gan.summary()\n",
        "\n",
        "# %% [code]\n",
        "# saved_GAN_location = \"Output/places2_final_k_1_with_noise/GAN_Weights_Epoch_100.h5\"\n",
        "print(saved_GAN_location)\n",
        "print(saved_D_location)\n",
        "\n",
        "# %% [code]\n",
        "# Load the weights\n",
        "discriminator.load_weights(saved_D_location)\n",
        "gan.load_weights(saved_GAN_location)\n",
        "\n",
        "print(\"Model loaded\")\n",
        "\n",
        "# %% [code]\n",
        "noise = generate_noise(len(l_channel_imgs), 100)\n",
        "\n",
        "# colorized_predictions is [num_test_imgs, k]\n",
        "colorized_predictions = generator.predict([noise, l_channel_imgs])\n",
        "\n",
        "print(\"Predictions for\", len(colorized_predictions), \"images complete!\")\n",
        "\n",
        "# %% [code] {\"scrolled\":true}\n",
        "save_path\n",
        "\n",
        "# %% [code]\n",
        "if random_select_gt_or_colorzed:\n",
        "    # Make models so we can determine best prediction given discrim value\n",
        "    print(\"Building models...\")\n",
        "    models = make_discrim_models()\n",
        "\n",
        "    # Get discrim values and reshape\n",
        "    print(\"Making discrim predictions...\")\n",
        "    discrim_values = discriminator.predict([colorized_predictions, l_channel_imgs])\n",
        "    print(\"Predictions complete!\\n\")\n",
        "    n = discrim_values.shape\n",
        "    reshaped_predictions_discrim = discrim_values.reshape((n[0], n[1]*n[2], n[3]))\n",
        "\n",
        "    # Get probabilities for each k\n",
        "    print(\"Testing each k...\")\n",
        "    probs = []\n",
        "    for i in range(k):\n",
        "        k_discrim_value = reshaped_predictions_discrim[:,:,i]\n",
        "        prob = models[i].predict_proba(k_discrim_value)\n",
        "        probs.append(prob[:,1])\n",
        "        print(\"k =\", i, \"complete!\")\n",
        "\n",
        "    # Store best prediction index\n",
        "    best_prediction_indices = np.argmax(np.array(probs), 0)\n",
        "\n",
        "    print('best_prediction_indices:', best_prediction_indices)\n",
        "\n",
        "# %% [code] {\"scrolled\":true}\n",
        "print(\"Merging, deprocessing, converting to RGB, and saving images\")\n",
        "\n",
        "num_ground_truth_selected = 0\n",
        "original_img_names = list_image_files(test_dataset)\n",
        "\n",
        "# Loop through images that were colorized\n",
        "for i, img in enumerate(colorized_predictions):\n",
        "    if preserve_img_names:\n",
        "        img_name = original_img_names[i]\n",
        "        filename = img_name.split(\"/\")[-1].split(\".\")[0]\n",
        "    else:\n",
        "        filename = str(i+1).zfill(len(str(num_test_imgs)))\n",
        "\n",
        "    # For each img, use either ground truth or random colorized prediction\n",
        "    if random_select_gt_or_colorzed:\n",
        "        # Determine whether to use ground truth or prediction\n",
        "        use_ground_truth = random.choice([True, False])\n",
        "\n",
        "        # Either use ground truth or best prediction\n",
        "        if use_ground_truth:\n",
        "            num_ground_truth_selected += 1\n",
        "            filename += \"_Ground_Truth.png\"\n",
        "            save_rgb_img(l_channel_imgs[i], ab_channel_imgs[i], filename)\n",
        "        else:\n",
        "            prediction_i = best_prediction_indices[i]\n",
        "            filename += \"_Colorized_.png\"\n",
        "            prediction = img[:,:,2*prediction_i:2*prediction_i+2]\n",
        "            save_rgb_img(l_channel_imgs[i], prediction, filename)\n",
        "    else:\n",
        "        # Save original\n",
        "        save_rgb_img(l_channel_imgs[i], ab_channel_imgs[i], filename + \" original.png\")\n",
        "        # Save grayscale\n",
        "        gray = np.zeros((256, 256, 2))\n",
        "        save_rgb_img(l_channel_imgs[i], gray, filename + \" grayscale.png\")\n",
        "\n",
        "        # Loop through predictions\n",
        "        for j in range(k):\n",
        "            prediction = img[:,:,2*j:2*j+2]\n",
        "            save_rgb_img(l_channel_imgs[i], prediction, filename + \"-\" + str(j+1) + \".png\")\n",
        "\n",
        "    if (i + 1) % 25 == 0:\n",
        "        print(\"--\", i+1, \"completed\")\n",
        "\n",
        "print(\"DONE!\")\n",
        "\n",
        "if random_select_gt_or_colorzed:\n",
        "    print(\"\\nBreakdown of Ground Truth vs. Colorized Selected:\")\n",
        "    print(\"Ground Truth:\", num_ground_truth_selected, \"--\", str(100 * num_ground_truth_selected / len(l_channel_imgs)) + \"%\")\n",
        "    print(\"Colorized:\", len(l_channel_imgs) - num_ground_truth_selected, \"--\", str(100 * (len(l_channel_imgs) - num_ground_truth_selected) / len(l_channel_imgs)) + \"%\")\n",
        "\n",
        "# %% [code] {\"scrolled\":true}\n",
        "# Troubleshooting discriminator values\n",
        "\n",
        "# for i, img in enumerate(discrim_values):\n",
        "#     print('=====', i, '=====')\n",
        "#     for j in range(k):\n",
        "#         print(j, sum(img[:,:,j]))\n",
        "# #         guess = colorized_predictions[i][:,:,2*j:2*j+2].astype(np.float64)\n",
        "# # #         prediction = cv2.merge((np.expand_dims(l_channel_imgs[i], -1), colorized_predictions[i][:,:,2*i:2*i+2]))\n",
        "# #         print(guess.shape)\n",
        "# #         merged = cv2.merge((l_channel_imgs[i], guess))\n",
        "# #         plt.imshow(cv2.cvtColor(deprocess_image(merged), cv2.COLOR_LAB2RGB))\n",
        "# #         plt.show()\n",
        "#     print(\"MAX:\",np.argmax(sum(img, axis=(0,1))))\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "# Each k has its own discrim model\n",
        "\n",
        "# pred = colorized_predictions[:1, :, :, :2]\n",
        "# print(pred.shape)\n",
        "\n",
        "# disp = deprocess_image(cv2.merge((l_channel_imgs[0], ab_channel_imgs[0])))\n",
        "# disp = cv2.cvtColor(disp, cv2.COLOR_LAB2RGB)\n",
        "# plt.figure()\n",
        "# plt.imshow(disp)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "# disp = deprocess_image(cv2.merge((l_channel_imgs[0], pred[0].astype(np.float64))))\n",
        "# disp = cv2.cvtColor(disp, cv2.COLOR_LAB2RGB)\n",
        "# plt.figure()\n",
        "# plt.imshow(disp)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "# three_of_same = np.zeros((1, 256, 256, 6))\n",
        "# three_of_same[:, :, :, :2] += pred\n",
        "# three_of_same[:, :, :, 2:4] += pred\n",
        "# three_of_same[:, :, :, 4:] += pred\n",
        "\n",
        "\n",
        "# discrim_score = discriminator.predict([three_of_same, l_channel_imgs[:1]])\n",
        "# for x in range(3):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(discrim_score[0, :, :, x])\n",
        "#     plt.colorbar()\n",
        "#     plt.show()"
      ],
      "metadata": {
        "_uuid": "4427ddf1-9c76-4703-9e63-3be07e062559",
        "_cell_guid": "491c4b43-fe0c-4fc7-aa1e-930e65c37562",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "RTS-pqY6Jjz2"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}